{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex and Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple notebook largely inspired by Bhavik Jikadara in his post on medium here: https://bhavikjikadara.medium.com/llamaindex-chroma-building-a-simple-rag-pipeline-cd67fc184190\n",
    "\n",
    "This is a one stop notebook for those that just want to test a simple RAG use case using chroma and llama3 (using Ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Retrieval-Augmented Generation (RAG)?\n",
    "RAG (Retrieval-Augmented Generation) is a technique in natural language processing (NLP) that enhances the capabilities of language models by combining information retrieval with text generation. It allows a model to generate more accurate and contextually relevant responses by retrieving relevant pieces of information from a database or knowledge base before generating text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*izQyDNIbCsyx8YI48avntA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Indexing\n",
    "The first step in building a RAG pipeline is data indexing. This process involves converting text data into a searchable database of vector embeddings, which represent the meaning of the text in a format that computers can easily understand.\n",
    "\n",
    "- Document Chunking: The collection of documents is split into smaller chunks of text. This allows for more precise and relevant pieces of information to be fed into the language model when needed, avoiding information overload.\n",
    "- Vector Embeddings: The chunks of text are then transformed into vector embeddings. These embeddings encode the meaning of natural language text into numerical representations.\n",
    "- Vector Database: Finally, the vector embeddings are stored in a vector database, making them easily searchable.\n",
    "\n",
    "## 2. Data Retrieval and Generation\n",
    "Once the context data is stored as vector embeddings, the process of data retrieval and generation begins.\n",
    "\n",
    "- Query Transformation: The user’s query (or prompt) is also transformed into a vector embedding, similar to how the context data was processed.\n",
    "- Context Matching: The query vector is compared against all the vectors in the vector database. The top-k most similar chunks of context data are selected.\n",
    "- Response Generation: The selected chunks of context, along with the user’s query, are fed into the language model (LLM) to generate a relevant and accurate response.\n",
    "\n",
    "## How to Built a Simple RAG Pipeline\n",
    "Now that we’ve covered the theory behind a RAG pipeline, let’s dive into the practical implementation. Below are the steps we’ll follow:\n",
    "\n",
    "- Set up the environment\n",
    "- Import an LLM\n",
    "- Import an embedding model\n",
    "- Prepare the data\n",
    "- Prompt Engineering\n",
    "- Create the query engine\n",
    "- Setting Up the Environment\n",
    "\n",
    "First, we need to import the necessary libraries:\n",
    "\n",
    "- **Chroma**: An AI-native open-source vector database, which will be used to create a vector database for our embeddings.\n",
    "- **LlamaIndex**: A framework for building context-augmented generative AI applications with LLMs. It handles reading the context data, creating vector embeddings, building prompt templates, and prompting the LLM locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install these libraries, you can run the following commands:\n",
    "!pip install chromadb llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama index comes \"naked\", which means that to use chroma(the vectorDb), Llama (the LLM through Ollama) and the embedder for the text (through huggingface), we need to install the specific 'Flavour' of llama-index for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-llms-ollama\n",
    "%pip install llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import chromadb\n",
    "from llama_index.core import PromptTemplate, Settings, SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama uses a server-client architecture where models must be pulled (downloaded) and loaded onto the server before they can be used.\n",
    "\n",
    "Here’s how you can automate the entire process in a Jupyter notebook, from starting the server to pulling the model and making requests:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_ollama_installed():\n",
    "    \"\"\"\n",
    "    Check if Ollama is installed on the system.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run([\"ollama\", \"--version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Ollama is installed: {result.stdout.decode().strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Ollama is not installed.\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ollama is not installed.\")\n",
    "        return False\n",
    "\n",
    "def install_ollama():\n",
    "    \"\"\"\n",
    "    Install Ollama using the official installation script.\n",
    "    \"\"\"\n",
    "    print(\"Installing Ollama...\")\n",
    "    try:\n",
    "        # Run the installation script\n",
    "        subprocess.run(\n",
    "            [\"curl\", \"-fsSL\", \"https://ollama.com/install.sh\", \"|\", \"sh\"],\n",
    "            check=True,\n",
    "            shell=True,\n",
    "        )\n",
    "        print(\"Ollama installation completed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred during installation: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def verify_installation():\n",
    "    \"\"\"\n",
    "    Verify that Ollama is installed and available.\n",
    "    \"\"\"\n",
    "    if is_ollama_installed():\n",
    "        print(\"Ollama is ready to use.\")\n",
    "    else:\n",
    "        print(\"Failed to verify Ollama installation. Please check manually.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if not is_ollama_installed():\n",
    "    install_ollama()\n",
    "verify_installation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Llama LLM\n",
    "\n",
    "With the libraries imported, we can now bring in the Llama language model. I opted for Llama because it allows for local execution, which is both free and private. Using the Ollama library makes it simple:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Ollama server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def start_ollama_server():\n",
    "    try:\n",
    "        subprocess.Popen([\"ollama\", \"serve\"])\n",
    "        print(\"Ollama server started.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Ollama is not installed. Please install it first.\")\n",
    "\n",
    "def pull_model(model_name):\n",
    "    try:\n",
    "        subprocess.run([\"ollama\", \"pull\", model_name], check=True)\n",
    "        print(f\"Model '{model_name}' pulled successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to pull model '{model_name}': {e}\")\n",
    "\n",
    "def is_server_running(base_url):\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "model_name = \"llama3\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "\n",
    "# Ensure server is running\n",
    "if not is_server_running(base_url):\n",
    "    start_ollama_server()\n",
    "    time.sleep(5)\n",
    "\n",
    "# Pull the model\n",
    "pull_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and we test it out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=model_name, base_url=base_url)\n",
    "response = llm.complete(\"Why is the sky blue?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need an embedding model to transform text into vector embeddings. I chose the **“BAAI/bge-small-en-v1.5”** model from Hugging Face, which is small and quick to implement — ideal for a proof of concept (POC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "\"\"\"\n",
    "If you want something fancier or are versed in chineses, here are the available models that can be used\n",
    "    BGE_MODELS = (\n",
    "        \"BAAI/bge-small-en\",\n",
    "        \"BAAI/bge-small-en-v1.5\",\n",
    "        \"BAAI/bge-base-en\",\n",
    "        \"BAAI/bge-base-en-v1.5\",\n",
    "        \"BAAI/bge-large-en\",\n",
    "        \"BAAI/bge-large-en-v1.5\",\n",
    "        \"BAAI/bge-small-zh\",\n",
    "        \"BAAI/bge-small-zh-v1.5\",\n",
    "        \"BAAI/bge-base-zh\",\n",
    "        \"BAAI/bge-base-zh-v1.5\",\n",
    "        \"BAAI/bge-large-zh\",\n",
    "        \"BAAI/bge-large-zh-v1.5\",\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "To prepare the data, we first read the context file using SimpleDirectoryReader. In this example, we're using a PDF of my one-page resume. We then create a vector database using Chroma and store the vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.api\n",
    "import chromadb.api.client\n",
    "\n",
    "\n",
    "def collection_exist(chroma_client: chromadb.api.client.Client, name: str ):\n",
    "    collection_list = chroma_client.list_collections()\n",
    "    collection_name_set = set((item.name for item in collection_list))\n",
    "    return name in collection_name_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"../data/external/Resume_Maxime_Bonnesoeur.pdf\"]).load_data()\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "if not collection_exist(chroma_client=chroma_client, name = \"ollama\"):\n",
    "    chroma_collection = chroma_client.create_collection(\"ollama\")\n",
    "else:\n",
    "    chroma_collection = chroma_client.get_collection(\"ollama\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    storage_context=storage_context, \n",
    "    embed_model=embed_model,\n",
    "    transformations=[SentenceSplitter(chunk_size=256, chunk_overlap=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "With the RAG pipeline set up, the next step is writing a template query. This template assigns the LLM a task and persona, provides context, and plugs in the user’s question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"Imagine you are a data scientist's assistant and \"\n",
    "    \"you answer a recruiter's questions about the data scientist's experience.\"\n",
    "    \"Here is some context from the data scientist's \"\n",
    "    \"resume related to the query::\\n\"\n",
    "    \"-----------------------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-----------------------------------------\\n\"\n",
    "    \"Considering the above information, \"\n",
    "    \"Please respond to the following inquiry:\\n\\n\"\n",
    "    \"Question: {query_str}\\n\\n\"\n",
    "    \"Answer succinctly and ensure your response is \"\n",
    "    \"clear to someone without a data science background.\"\n",
    "    \"The data scientist's name is in the document.\"\n",
    ")\n",
    "qa_template = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the query engine that will run all of the components of our RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    text_qa_template=qa_template,\n",
    "    similarity_top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the RAG Pipeline\n",
    "The exciting part of building an AI application is seeing it work! To run the RAG pipeline, simply prompt the query engine with a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Do you have experience with Python?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
