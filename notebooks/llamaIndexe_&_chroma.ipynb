{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex and Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple notebook largely inspired by Bhavik Jikadara in his post on medium here: https://bhavikjikadara.medium.com/llamaindex-chroma-building-a-simple-rag-pipeline-cd67fc184190\n",
    "\n",
    "This is a one stop notebook for those that just want to test a simple RAG use case using chroma and llama3 (using Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install these libraries, you can run the following commands:\n",
    "!pip install chromadb llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-llms-ollama\n",
    "%pip install llama-index-vector-stores-chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import chromadb\n",
    "from llama_index.core import PromptTemplate, Settings, SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intall Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_ollama_installed():\n",
    "    \"\"\"\n",
    "    Check if Ollama is installed on the system.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run([\"ollama\", \"--version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Ollama is installed: {result.stdout.decode().strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Ollama is not installed.\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ollama is not installed.\")\n",
    "        return False\n",
    "\n",
    "def install_ollama():\n",
    "    \"\"\"\n",
    "    Install Ollama using the official installation script.\n",
    "    \"\"\"\n",
    "    print(\"Installing Ollama...\")\n",
    "    try:\n",
    "        # Run the installation script\n",
    "        subprocess.run(\n",
    "            [\"curl\", \"-fsSL\", \"https://ollama.com/install.sh\", \"|\", \"sh\"],\n",
    "            check=True,\n",
    "            shell=True,\n",
    "        )\n",
    "        print(\"Ollama installation completed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred during installation: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def verify_installation():\n",
    "    \"\"\"\n",
    "    Verify that Ollama is installed and available.\n",
    "    \"\"\"\n",
    "    if is_ollama_installed():\n",
    "        print(\"Ollama is ready to use.\")\n",
    "    else:\n",
    "        print(\"Failed to verify Ollama installation. Please check manually.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if not is_ollama_installed():\n",
    "    install_ollama()\n",
    "verify_installation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Llama LLM\n",
    "\n",
    "With the libraries imported, we can now bring in the Llama language model. I opted for Llama because it allows for local execution, which is both free and private. Using the Ollama library makes it simple:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Ollama server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def start_ollama_server():\n",
    "    try:\n",
    "        subprocess.Popen([\"ollama\", \"serve\"])\n",
    "        print(\"Ollama server started.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Ollama is not installed. Please install it first.\")\n",
    "\n",
    "def pull_model(model_name):\n",
    "    try:\n",
    "        subprocess.run([\"ollama\", \"pull\", model_name], check=True)\n",
    "        print(f\"Model '{model_name}' pulled successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to pull model '{model_name}': {e}\")\n",
    "\n",
    "def is_server_running(base_url):\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "model_name = \"llama3\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "\n",
    "# Ensure server is running\n",
    "if not is_server_running(base_url):\n",
    "    start_ollama_server()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the model\n",
    "pull_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=model_name, base_url=base_url)\n",
    "response = llm.complete(\"Why is the sky blue?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need an embedding model to transform text into vector embeddings. I chose the “BAAI/bge-small-en-v1.5” model from Hugging Face, which is small and quick to implement — ideal for a proof of concept (POC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "\"\"\"\n",
    "If you want something fancier or are versed in chineses, here are the available models that can be used\n",
    "    BGE_MODELS = (\n",
    "        \"BAAI/bge-small-en\",\n",
    "        \"BAAI/bge-small-en-v1.5\",\n",
    "        \"BAAI/bge-base-en\",\n",
    "        \"BAAI/bge-base-en-v1.5\",\n",
    "        \"BAAI/bge-large-en\",\n",
    "        \"BAAI/bge-large-en-v1.5\",\n",
    "        \"BAAI/bge-small-zh\",\n",
    "        \"BAAI/bge-small-zh-v1.5\",\n",
    "        \"BAAI/bge-base-zh\",\n",
    "        \"BAAI/bge-base-zh-v1.5\",\n",
    "        \"BAAI/bge-large-zh\",\n",
    "        \"BAAI/bge-large-zh-v1.5\",\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.api\n",
    "import chromadb.api.client\n",
    "\n",
    "\n",
    "def collection_exist(chroma_client: chromadb.api.client.Client, name: str ):\n",
    "    collection_list = chroma_client.list_collections()\n",
    "    collection_name_set = set((item.name for item in collection_list))\n",
    "    return name in collection_name_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"../data/external/Resume_Maxime_Bonnesoeur.pdf\"]).load_data()\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "if not collection_exist(chroma_client=chroma_client, name = \"ollama\"):\n",
    "    chroma_collection = chroma_client.create_collection(\"ollama\")\n",
    "else:\n",
    "    chroma_collection = chroma_client.get_collection(\"ollama\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    storage_context=storage_context, \n",
    "    embed_model=embed_model,\n",
    "    transformations=[SentenceSplitter(chunk_size=256, chunk_overlap=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"Imagine you are a data scientist's assistant and \"\n",
    "    \"you answer a recruiter's questions about the data scientist's experience.\"\n",
    "    \"Here is some context from the data scientist's \"\n",
    "    \"resume related to the query::\\n\"\n",
    "    \"-----------------------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-----------------------------------------\\n\"\n",
    "    \"Considering the above information, \"\n",
    "    \"Please respond to the following inquiry:\\n\\n\"\n",
    "    \"Question: {query_str}\\n\\n\"\n",
    "    \"Answer succinctly and ensure your response is \"\n",
    "    \"clear to someone without a data science background.\"\n",
    "    \"The data scientist's name is Maxime Bonnesoeur.\"\n",
    ")\n",
    "qa_template = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    text_qa_template=qa_template,\n",
    "    similarity_top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Do you have experience with Python?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
